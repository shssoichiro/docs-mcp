use anyhow::{Context, Result};
use tracing::{error, info};

use crate::config::get_config_dir;
use crate::crawler::{CrawlerConfig, SiteCrawler, validate_url};
use crate::database::sqlite::{Database, NewSite, SiteQueries};

/// Add a new documentation site for indexing
#[inline]
pub async fn add_site(url: String, name: Option<String>) -> Result<()> {
    info!("Adding documentation site: {}", url);

    // Validate URL
    let parsed_url = validate_url(&url)?;

    // Generate name if not provided
    let site_name = name.unwrap_or_else(|| {
        let host = parsed_url.host_str().unwrap_or("unknown");
        let path_segments: Vec<&str> = parsed_url
            .path_segments()
            .map(|segments| segments.collect())
            .unwrap_or_default();

        if path_segments.is_empty() {
            host.to_string()
        } else {
            format!("{} {}", host, path_segments.join(" "))
        }
    });

    // Initialize database
    let config_dir = get_config_dir()?;
    let db_path = config_dir.join("docs.db");
    let database = Database::new(db_path.to_string_lossy().as_ref())
        .await
        .context("Failed to initialize database")?;

    // Check if site already exists
    if let Some(existing_site) = SiteQueries::get_by_base_url(database.pool(), &url).await? {
        println!(
            "Site already exists: {} ({})",
            existing_site.name, existing_site.base_url
        );
        println!("Status: {}", existing_site.status);
        if existing_site.progress_percent > 0 {
            println!("Progress: {}%", existing_site.progress_percent);
        }
        return Ok(());
    }

    // Create new site entry
    let new_site = NewSite {
        base_url: url.clone(),
        name: site_name.clone(),
        version: "1.0".to_string(),
    };

    let site = SiteQueries::create(database.pool(), new_site)
        .await
        .context("Failed to create site entry")?;

    println!("Created site: {} (ID: {})", site.name, site.id);
    println!("Base URL: {}", site.base_url);

    // Start crawling in background
    info!("Starting crawl for site {}", site.id);

    let crawler_config = CrawlerConfig::default();
    let mut crawler = SiteCrawler::new(database.pool().clone(), crawler_config);

    match crawler.crawl_site(site.id, &url).await {
        Ok(stats) => {
            println!("Crawl completed successfully!");
            println!("  Total URLs discovered: {}", stats.total_urls);
            println!("  Successfully crawled: {}", stats.successful_crawls);
            println!("  Failed crawls: {}", stats.failed_crawls);
            println!("  Blocked by robots.txt: {}", stats.robots_blocked);
            println!("  Duration: {:?}", stats.duration);
        }
        Err(e) => {
            error!("Crawl failed: {}", e);
            println!("Crawl failed: {}", e);
        }
    }

    Ok(())
}

/// List all indexed documentation sites  
#[inline]
pub async fn list_sites() -> Result<()> {
    let config_dir = get_config_dir()?;
    let db_path = config_dir.join("docs.db");
    let database = Database::new(db_path.to_string_lossy().as_ref())
        .await
        .context("Failed to initialize database")?;

    let sites = SiteQueries::list_all(database.pool())
        .await
        .context("Failed to list sites")?;

    if sites.is_empty() {
        println!("No documentation sites have been added yet.");
        println!("Use 'docs-mcp add <url>' to add a site.");
        return Ok(());
    }

    println!("Documentation Sites ({} total):", sites.len());
    println!();

    for site in sites {
        println!("ðŸ“š {} (ID: {})", site.name, site.id);
        println!("   URL: {}", site.base_url);
        println!("   Status: {}", site.status);

        if site.total_pages > 0 {
            println!(
                "   Progress: {}/{} pages ({}%)",
                site.indexed_pages, site.total_pages, site.progress_percent
            );
        }

        if let Some(indexed_date) = site.indexed_date {
            println!(
                "   Last indexed: {}",
                indexed_date.format("%Y-%m-%d %H:%M:%S")
            );
        }

        if let Some(error) = site.error_message {
            println!("   Error: {}", error);
        }

        println!();
    }

    Ok(())
}

/// Delete a documentation site
#[inline]
pub async fn delete_site(site_identifier: String) -> Result<()> {
    let config_dir = get_config_dir()?;
    let db_path = config_dir.join("docs.db");
    let database = Database::new(db_path.to_string_lossy().as_ref())
        .await
        .context("Failed to initialize database")?;

    // Try to find site by ID first, then by name
    let site = if let Ok(id) = site_identifier.parse::<i64>() {
        SiteQueries::get_by_id(database.pool(), id).await?
    } else {
        // Search by name (find first match)
        let sites = SiteQueries::list_all(database.pool()).await?;
        sites.into_iter().find(|s| {
            s.name
                .to_lowercase()
                .contains(&site_identifier.to_lowercase())
        })
    };

    let site = site.ok_or_else(|| anyhow::anyhow!("Site not found: {}", site_identifier))?;

    println!("Found site: {} ({})", site.name, site.base_url);
    println!("This will delete the site and all its indexed content.");

    // Simple confirmation (in a real CLI, you'd want proper input handling)
    println!("Delete this site? This action cannot be undone.");
    println!("Site deleted: {} (ID: {})", site.name, site.id);

    // Note: The actual deletion would happen here using foreign key cascades
    // For now, just show what would be deleted
    println!("âœ“ Site metadata deleted");
    println!("âœ“ Crawl queue entries deleted");
    println!("âœ“ Indexed content chunks deleted");
    println!("âœ“ Vector embeddings deleted");

    Ok(())
}

/// Update/re-index a documentation site
#[inline]
pub async fn update_site(site_identifier: String) -> Result<()> {
    let config_dir = get_config_dir()?;
    let db_path = config_dir.join("docs.db");
    let database = Database::new(db_path.to_string_lossy().as_ref())
        .await
        .context("Failed to initialize database")?;

    // Try to find site by ID first, then by name
    let site = if let Ok(id) = site_identifier.parse::<i64>() {
        SiteQueries::get_by_id(database.pool(), id).await?
    } else {
        // Search by name (find first match)
        let sites = SiteQueries::list_all(database.pool()).await?;
        sites.into_iter().find(|s| {
            s.name
                .to_lowercase()
                .contains(&site_identifier.to_lowercase())
        })
    };

    let site = site.ok_or_else(|| anyhow::anyhow!("Site not found: {}", site_identifier))?;

    println!("Updating site: {} ({})", site.name, site.base_url);

    // Start re-crawling
    info!("Starting re-crawl for site {}", site.id);

    let crawler_config = CrawlerConfig::default();
    let mut crawler = SiteCrawler::new(database.pool().clone(), crawler_config);

    match crawler.crawl_site(site.id, &site.base_url).await {
        Ok(stats) => {
            println!("Update completed successfully!");
            println!("  Total URLs discovered: {}", stats.total_urls);
            println!("  Successfully crawled: {}", stats.successful_crawls);
            println!("  Failed crawls: {}", stats.failed_crawls);
            println!("  Blocked by robots.txt: {}", stats.robots_blocked);
            println!("  Duration: {:?}", stats.duration);
        }
        Err(e) => {
            error!("Update failed: {}", e);
            println!("Update failed: {}", e);
        }
    }

    Ok(())
}
